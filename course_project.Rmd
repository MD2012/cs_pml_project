---
title: "Practical Machine Learning - Weight Lifting Exercise Prediction"
date: "28. Februar 2016"
output: html_document
---

# Experiment & Task
Six participants were asked to perform one set of 10 repetitions of the Unilateral Dumbbell Biceps Curl in five different fashions:

| Class A | Class B | Class C | Class D | Class E |
|---|---|---|---|---|
exactly according to specification | throwing elbows to the front | lifting the dumbbell only halfway | lowering the dumbbell only halfway | throwing the hips to the front

The task is to predict the manner in which people did the Unilateral Dumbbell Biceps Curl exercise in a test set using the best fit model, fitted on the "classe" variable of the training set. We shall outline how the model is build, conduct cross-validation of our results and discuss the accuracy of the model.

```{r,verbose=FALSE,echo=FALSE,message=FALSE,warning=FALSE,results='hide'}
library(ggplot2);
library(caret);
library(rpart)
library(rpart.plot)
library(rattle)
library(randomForest)
set.seed(12345)
#library(Rcpp)
#library(Amelia)
```

## Load
First of all, we load the dataset. Since we do not want to download the files on each analysis run we
follow a 2-step process. First, if the files have not been downloaded to the local folder we download them.
In any subsequent analysis run (such as the analysis at hand) , the files will not be re-downloaded as they are already available in the local folder.
```{r}
src_train <- read.csv("pml-training.csv")
src_test <- read.csv("pml-testing.csv")
```

## Explore
Here we get a grasp on the overall dataset, its structure and summary statistics.
```{r}
names(src_train)
dim(src_train)
summary(src_train)
```
Due to the big amount of 160 variables included, we decide against further exploratory analysis to detect correlation structures using featureplot e.g. but will directly step into preparation of machine learning model fits in the next steps.

## Clean
Now we turn to cleaning the data from:

1. Irrelevant Variables
2. Near Zero Variance Variables
3. Variables with too many N/A values

### Remove Irrelevant
Looking at the variables we do not expect that neither the running ID variable **X**, the **user_name** nor any of the timestamp variables  have a significant influence on the outcome variable **classe** unless the experiment setup is flawed itself.
```{r}
clean_irrelevant <- c("X","user_name","raw_timestamp_part_1","raw_timestamp_part_2","cvtd_timestamp")
clean_names <- names(src_train) %in% clean_irrelevant
src_train_cleaned <- src_train[!clean_names]
src_test_cleaned <- src_test[!clean_names]
dim(src_train_cleaned)
dim(src_test_cleaned)
```

The Training and Test data sets were shrinked by 5 variables.

### Remove N/As
Now we set up an algorithm to either impute or remove variables with too many n/a values.
The summary table revealed that some columns either have no n/a values or a large amount.
Hence we decide to remove all columns that contain more than 30% n/a values:
```{r}
totRows <- nrow(src_train_cleaned);
naFraction <- 0.3;
totNaFraction <- naFraction * totRows;
stc <- src_train_cleaned;
src_train_cleaned <- src_train_cleaned[, colSums(is.na(stc)) < totNaFraction];
src_test_cleaned <- src_test_cleaned[, colSums(is.na(stc)) < totNaFraction];
dim(src_train_cleaned)
dim(src_test_cleaned)
```

The Training and Test data sets were shrinked by further 67 variables.

### Remove Near Zero Variance
Variables that have near zero variance are also removed.
```{r}
clean_nzv <- nearZeroVar(src_train_cleaned, saveMetrics=TRUE)
src_train_cleaned <- src_train_cleaned[!clean_nzv$nzv]
src_test_cleaned <- src_test_cleaned[!clean_nzv$nzv]
dim(src_train_cleaned)
dim(src_test_cleaned)
```

The Training and Test data sets were shrinked by further 34 variables.

## Partition
In order to train multiple models we further partition the training test set into a 70% training and 30% validation set such that we arrive at **train**, a **validation** and a **test** data subset.
```{r}
partThreshold <- 0.7;
inTrain <- createDataPartition(y=src_train_cleaned$classe, p=partThreshold, list=FALSE);
train <- src_train_cleaned[inTrain,];
valid <- src_train_cleaned[-inTrain,];
test <- src_test_cleaned;
dim(train);
dim(valid);
dim(test);
#names(train);
#names(test);
```

## Check
Now we perform some basic sanity check on the data sets, i.e.:

1. Does the column number match in all data subsets?
```{r}
match_cols <- all.equal(ncol(train), ncol(valid), ncol(test));
match_cols
```

## Fit
Using the **train** data we will now fit **2** different models, a decision tree and a random forest based model.

### Decision Tree
We are now applying the rpart model fit on our cleaned train data set and creating the corresponding fancyRpart plot to visualize the result decision tree.
```{r}
modFit1 <- rpart(classe ~ ., data=train)
fancyRpartPlot(modFit1)
```

We can now use the result decision tree to predict **classe** based on **modFit1**:
```{r}
pred1 <- predict(modFit1, newdata=valid, type="class")
```

### Random Forest
Now we run the same analysis using a random forest algorithm:
```{r}
modFit2 <- randomForest(classe ~ ., data=train)
pred2 <- predict(modFit2, newdata=valid, type="class")
```

## Analyse
Looking at the **confusionMatrix accuracy** we would favor the random forest algorithm over the decision tree based approach:
```{r}
confusionMatrix(pred1, valid$classe)
confusionMatrix(pred2, valid$classe)
varImpPlot(modFit2, main="Variable Importance: Random Forest", n.var = 50)
```

The Variable Importance Chart also underlines that the random model at hand explains a considerable amount of variance with about 50 variables included.